{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "95f69f4171822e9a0d4258a52ad5b962ce921882fa0fb93c07eb68a8992e9dab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "cuda_available = torch.cuda.is_available()\n",
    "import wandb\n",
    "from simpletransformers.ner import NERModel, NERArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"..\\..\\Dataset\\\\bert\\\\train.csv\")\n",
    "eval_df=pd.read_csv(\"..\\..\\Dataset\\\\bert\\\\val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                sentence_id     words labels\n",
       "0       1382601382042103808     Hidup      O\n",
       "1       1382601382042103808   sesedih      O\n",
       "2       1382601382042103808       dan      O\n",
       "3       1382601382042103808   secaper      O\n",
       "4       1382601382042103808       apa      O\n",
       "...                     ...       ...    ...\n",
       "108840  1383795163412058114  Gubernur      O\n",
       "108841  1383795163412058114  Khofifah  B-PER\n",
       "108842  1383795163412058114     fokus      O\n",
       "108843  1383795163412058114     untuk      O\n",
       "108844  1383795163412058114  pulihkan      O\n",
       "\n",
       "[108845 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>words</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1382601382042103808</td>\n      <td>Hidup</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1382601382042103808</td>\n      <td>sesedih</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1382601382042103808</td>\n      <td>dan</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1382601382042103808</td>\n      <td>secaper</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1382601382042103808</td>\n      <td>apa</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>108840</th>\n      <td>1383795163412058114</td>\n      <td>Gubernur</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>108841</th>\n      <td>1383795163412058114</td>\n      <td>Khofifah</td>\n      <td>B-PER</td>\n    </tr>\n    <tr>\n      <th>108842</th>\n      <td>1383795163412058114</td>\n      <td>fokus</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>108843</th>\n      <td>1383795163412058114</td>\n      <td>untuk</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>108844</th>\n      <td>1383795163412058114</td>\n      <td>pulihkan</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>108845 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\", \n",
    "    \"B-PER\", \"I-PER\",\n",
    "    \"B-ORG\", \"I-ORG\",\n",
    "    \"B-LOC\", \"I-LOC\",\n",
    "    \"B-PROD\", \"I-PROD\",\n",
    "    \"B-WA\", \"I-WA\",\n",
    "    \"B-EV\", \"I-EV\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a NERModel\n",
    "model_args=NERArgs()\n",
    "# model_args.train_batch_size=64\n",
    "# model_args.overwrite_output_dir=True\n",
    "model_args = {\n",
    "    \"output_dir\": \"outputs/\",\n",
    "    \"cache_dir\": \"cache_dir/\",\n",
    "\n",
    "    \"fp16\": True,\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"eval_batch_size\": 8,\n",
    "    # \"num_train_epochs\": 1,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"weight_decay\": 0,\n",
    "    # \"learning_rate\": 4e-5,\n",
    "    \"learning_rate\": 0.00006828,\n",
    "\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_steps\": 2000,\n",
    "\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"reprocess_input_data\": False,\n",
    "    \"evaluate_during_training\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # grid, random\n",
    "    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"num_train_epochs\": {\"values\": [2, 3, 5]},\n",
    "        \"learning_rate\": {\"min\": 5e-5, \"max\": 4e-4},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create sweep with ID: dpnikvql\nSweep URL: https://wandb.ai/anwarfarihin/ner%20sweep/sweeps/dpnikvql\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"ner sweep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init()\n",
    "    model = NERModel('bert', \n",
    "                    'indolem/indobert-base-uncased',\n",
    "                    args=model_args,\n",
    "                    labels=label_list,\n",
    "                    use_cuda=cuda_available,\n",
    "                    sweep_config=wandb.config\n",
    "                    )\n",
    "    model.train_model(train_df, eval_df=eval_df)\n",
    "    model.eval_model(eval_df)\n",
    "    wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fev3o68z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0014197744241588898\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manwarfarihin\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">misty-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn</a><br/>\nRun page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/fev3o68z\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/fev3o68z</a><br/>\n                Run data is saved locally in <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172755-fev3o68z</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 of 5:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 5:   0%|          | 0/722 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.6575:   0%|          | 0/722 [00:00<?, ?it/s]\u001b[Ad:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\simpletransformers\\ner\\ner_model.py:739: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  model.parameters(), args.max_grad_norm\n",
      "d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\n",
      "Epochs 0/5. Running Loss:    2.6575:   0%|          | 1/722 [00:00<06:04,  1.98it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.6479:   0%|          | 1/722 [00:00<06:04,  1.98it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.6479:   0%|          | 2/722 [00:00<05:42,  2.10it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.6425:   0%|          | 2/722 [00:01<05:42,  2.10it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.6425:   0%|          | 3/722 [00:01<05:29,  2.18it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.8280:   0%|          | 3/722 [00:01<05:29,  2.18it/s]\u001b[A\n",
      "Epochs 0/5. Running Loss:    2.8280:   1%|          | 4/722 [00:02<08:23,  1.43it/s]\n",
      "Epoch 1 of 5:   0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 10176<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5cf512c72184dbf884b6a23f4d8d9b0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172755-fev3o68z\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172755-fev3o68z\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">misty-sweep-1</strong>: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/fev3o68z\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/fev3o68z</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Run fev3o68z errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fev3o68z errored: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hp4i7bvf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00150582522846751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">silvery-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn</a><br/>\nRun page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/hp4i7bvf\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/hp4i7bvf</a><br/>\n                Run data is saved locally in <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172819-hp4i7bvf</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2944<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eccf2db37ae843599fccfdf36da00efd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172819-hp4i7bvf\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172819-hp4i7bvf\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">silvery-sweep-2</strong>: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/hp4i7bvf\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/hp4i7bvf</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Run hp4i7bvf errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hp4i7bvf errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c3dz6l1s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0038366880173895204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">fresh-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep</a><br/>\n                Sweep page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/sweeps/v993edbn</a><br/>\nRun page: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/c3dz6l1s\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/c3dz6l1s</a><br/>\n                Run data is saved locally in <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172835-c3dz6l1s</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 7036<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd4e623a7c0c464e9468a4045a5bed4f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172835-c3dz6l1s\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\Model\\bert\\wandb\\run-20210629_172835-c3dz6l1s\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">fresh-sweep-3</strong>: <a href=\"https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/c3dz6l1s\" target=\"_blank\">https://wandb.ai/anwarfarihin/Simple%20Sweep/runs/c3dz6l1s</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Run c3dz6l1s errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run c3dz6l1s errored: RuntimeError('CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 2.99 GiB reserved in total by PyTorch)')\n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}