{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "95f69f4171822e9a0d4258a52ad5b962ce921882fa0fb93c07eb68a8992e9dab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sklearn\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "import logging\r\n",
    "\r\n",
    "import torch\r\n",
    "cuda_available = torch.cuda.is_available()\r\n",
    "\r\n",
    "from simpletransformers.ner import NERModel, NERArgs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_df=pd.read_csv(\"..\\..\\..\\Dataset\\\\bert\\\\train.csv\")\r\n",
    "eval_df=pd.read_csv(\"..\\..\\..\\Dataset\\\\bert\\\\val.csv\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_df=train_df[:1000]\r\n",
    "train_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1382601382042103808</td>\n",
       "      <td>Hidup</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1382601382042103808</td>\n",
       "      <td>sesedih</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1382601382042103808</td>\n",
       "      <td>dan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1382601382042103808</td>\n",
       "      <td>secaper</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1382601382042103808</td>\n",
       "      <td>apa</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1388182147496767490</td>\n",
       "      <td>@jalilword</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1388182147496767490</td>\n",
       "      <td>Baik</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1388182147496767490</td>\n",
       "      <td>bagi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1388182147496767490</td>\n",
       "      <td>aku</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1388182147496767490</td>\n",
       "      <td>jadi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentence_id       words labels\n",
       "0    1382601382042103808       Hidup      O\n",
       "1    1382601382042103808     sesedih      O\n",
       "2    1382601382042103808         dan      O\n",
       "3    1382601382042103808     secaper      O\n",
       "4    1382601382042103808         apa      O\n",
       "..                   ...         ...    ...\n",
       "995  1388182147496767490  @jalilword      O\n",
       "996  1388182147496767490        Baik      O\n",
       "997  1388182147496767490        bagi      O\n",
       "998  1388182147496767490         aku      O\n",
       "999  1388182147496767490        jadi      O\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "eval_df=eval_df[:1000]\r\n",
    "eval_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1385573262067847169</td>\n",
       "      <td>Nadiem</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1385573262067847169</td>\n",
       "      <td>Makarim</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1385573262067847169</td>\n",
       "      <td>dg</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1385573262067847169</td>\n",
       "      <td>Presiden</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1385573262067847169</td>\n",
       "      <td>ke-5</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1387681409020530691</td>\n",
       "      <td>full</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1389427886491242498</td>\n",
       "      <td>GAMES</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1389427886491242498</td>\n",
       "      <td>juga</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1389427886491242498</td>\n",
       "      <td>loh</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1389427886491242498</td>\n",
       "      <td>!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentence_id     words labels\n",
       "0    1385573262067847169    Nadiem  B-PER\n",
       "1    1385573262067847169   Makarim  I-PER\n",
       "2    1385573262067847169        dg      O\n",
       "3    1385573262067847169  Presiden      O\n",
       "4    1385573262067847169      ke-5      O\n",
       "..                   ...       ...    ...\n",
       "995  1387681409020530691      full      O\n",
       "996  1389427886491242498     GAMES      O\n",
       "997  1389427886491242498      juga      O\n",
       "998  1389427886491242498       loh      O\n",
       "999  1389427886491242498         !      O\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "label_list = [\r\n",
    "    \"O\", \r\n",
    "    \"B-PER\", \"I-PER\",\r\n",
    "    \"B-ORG\", \"I-ORG\",\r\n",
    "    \"B-LOC\", \"I-LOC\",\r\n",
    "    \"B-PROD\", \"I-PROD\",\r\n",
    "    \"B-WA\", \"I-WA\",\r\n",
    "    \"B-EV\", \"I-EV\",\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Create a NERModel\r\n",
    "model_args=NERArgs()\r\n",
    "# model_args.train_batch_size=64\r\n",
    "# model_args.overwrite_output_dir=True\r\n",
    "model_args = {\r\n",
    "    \"output_dir\": \"outputs/\",\r\n",
    "    \"cache_dir\": \"cache_dir/\",\r\n",
    "\r\n",
    "    \"fp16\": True,\r\n",
    "    \"fp16_opt_level\": \"O1\",\r\n",
    "    \"max_seq_length\": 128,\r\n",
    "    \"train_batch_size\": 8,\r\n",
    "    \"gradient_accumulation_steps\": 1,\r\n",
    "    \"eval_batch_size\": 8,\r\n",
    "    \"num_train_epochs\": 1,\r\n",
    "    # \"num_train_epochs\": 5,\r\n",
    "    \"weight_decay\": 0,\r\n",
    "    \"learning_rate\": 4e-5,\r\n",
    "    # \"learning_rate\": 0.00006828,\r\n",
    "\r\n",
    "    \"adam_epsilon\": 1e-8,\r\n",
    "    \"warmup_ratio\": 0.06,\r\n",
    "    \"warmup_steps\": 0,\r\n",
    "    \"max_grad_norm\": 1.0,\r\n",
    "\r\n",
    "    \"logging_steps\": 50,\r\n",
    "    \"save_steps\": 2000,\r\n",
    "\r\n",
    "    \"overwrite_output_dir\": True,\r\n",
    "    \"reprocess_input_data\": False,\r\n",
    "    \"evaluate_during_training\": True,\r\n",
    "}\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# model = NERModel('bert', \r\n",
    "#                 'outputs',\r\n",
    "#                 args=model_args,\r\n",
    "#                 labels=label_list,\r\n",
    "#                 use_cuda=cuda_available,\r\n",
    "#                 )\r\n",
    "# # result, model_outputs, wrong_preds = model.eval_model(eval_df)\r\n",
    "# # result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = NERModel('bert', \r\n",
    "                'vinai/bertweet-large',\r\n",
    "                args=model_args,\r\n",
    "                labels=label_list,\r\n",
    "                use_cuda=cuda_available,\r\n",
    "                )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 614/614 [00:00<00:00, 205kB/s]\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Downloading: 100%|██████████| 1.42G/1.42G [04:01<00:00, 5.89MB/s]\n",
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing BertForTokenClassification: ['roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.12.attention.output.dense.bias', 'roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'roberta.encoder.layer.19.attention.self.key.bias', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.12.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.23.attention.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.16.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.self.value.bias', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.19.attention.self.value.weight', 'roberta.encoder.layer.15.attention.self.value.bias', 'roberta.encoder.layer.13.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.20.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.21.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.13.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.20.attention.self.query.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.15.attention.self.value.weight', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.12.intermediate.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.22.attention.self.query.bias', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.20.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.19.attention.self.query.weight', 'roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.22.attention.self.value.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.22.output.dense.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.23.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.20.attention.output.dense.weight', 'roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'roberta.encoder.layer.13.attention.self.key.bias', 'roberta.encoder.layer.18.attention.self.value.bias', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.16.attention.self.query.weight', 'roberta.encoder.layer.15.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.22.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.17.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.18.attention.output.dense.bias', 'roberta.encoder.layer.22.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.13.attention.self.query.weight', 'roberta.encoder.layer.16.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.20.attention.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.22.attention.self.value.bias', 'roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.17.attention.self.query.bias', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.16.attention.self.value.bias', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.23.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.23.attention.self.key.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.16.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.12.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'roberta.encoder.layer.21.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.21.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.23.attention.self.value.weight', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.20.attention.self.value.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.17.attention.self.value.bias', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.13.attention.self.key.weight', 'roberta.encoder.layer.14.attention.self.key.bias', 'roberta.encoder.layer.12.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.22.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.18.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.21.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.15.attention.self.query.bias', 'roberta.encoder.layer.16.attention.output.dense.weight', 'roberta.encoder.layer.19.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.17.attention.output.dense.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.self.query.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.15.attention.output.dense.bias', 'roberta.encoder.layer.17.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.17.attention.self.key.bias', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.17.attention.output.dense.weight', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.16.attention.self.key.weight', 'roberta.encoder.layer.23.attention.self.value.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.12.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.14.attention.self.query.weight', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.20.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.16.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.23.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.21.attention.output.dense.weight', 'roberta.encoder.layer.21.attention.self.key.weight', 'roberta.encoder.layer.14.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.14.attention.output.dense.weight', 'roberta.encoder.layer.13.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.19.attention.self.key.weight', 'roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.attention.self.query.weight', 'roberta.encoder.layer.20.attention.self.query.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.18.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.19.attention.self.value.bias', 'lm_head.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.23.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.15.attention.self.key.weight', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.12.attention.self.key.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.22.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.15.attention.output.dense.weight', 'roberta.encoder.layer.18.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.22.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.19.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.21.attention.self.key.bias', 'lm_head.decoder.bias', 'roberta.encoder.layer.18.attention.self.key.weight', 'roberta.encoder.layer.23.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.21.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.15.attention.self.key.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.14.attention.self.query.bias', 'roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.19.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.14.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.14.attention.self.value.weight', 'roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.21.output.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['encoder.layer.4.intermediate.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'classifier.weight', 'classifier.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.12.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:02<00:00, 660kB/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13984/449749419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0muse_cuda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda_available\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 )\n",
      "\u001b[1;32md:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\simpletransformers\\ner\\ner_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, labels, args, use_cuda, cuda_device, onnx_execution_provider, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             self.tokenizer = tokenizer_class.from_pretrained(\n\u001b[1;32m--> 343\u001b[1;33m                 \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             )\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m         return cls._from_pretrained(\n\u001b[1;32m-> 1720\u001b[1;33m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1721\u001b[0m         )\n\u001b[0;32m   1722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1790\u001b[0m         \u001b[1;31m# Instantiate tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m             raise OSError(\n",
      "\u001b[1;32md:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m             raise ValueError(\n\u001b[0;32m    195\u001b[0m                 \u001b[1;34mf\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "predictions, raw_outputs = model.predict(\r\n",
    "    # [\"@PolsekAlian PolriBerikan inovasi-inovasi Polri yang memudahkan masyarakat, kegiatan pemolisian masyarakat dan seluruh tindakan Kepolisian dalam menghadapi gangguan Kamtibmas, dan lain-lain - Jenderal Polisi Drs. Listyo Sigit Prabowo, https://t.co/f8GZ6VPqPD. - Kapolri #PolriTVRadioPresisi\"],\r\n",
    "    [\"Safari Ramadhan di Malang, Gubernur Khofifah ingatkan pentingnya Rembuk Nyekrup birokrat\"], \r\n",
    "    [\"@OPPOIndonesia ✍Butuh memori besar 128GB di OPPO A54 buat simpan file-file tugas kuliah dan testimoni online shop -ku ��Semoga oppo A54 bisa menunjang penghasilan keluargaku �� @OPPOIndonesia @nisa_rkt @Bebheey @kus_ica @Erna19_ @Saftriyuni #OPPOA54 https://t.co/bMYIiRb4QF �� 2.921\"]\r\n",
    "        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.cuda.empty_cache()\r\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.train_model(train_df,  eval_data=eval_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\n",
      "Epoch 1 of 1:   0%|          | 0/1 [00:00<?, ?it/s]d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\simpletransformers\\ner\\ner_model.py:739: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  model.parameters(), args.max_grad_norm\n",
      "d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epochs 0/1. Running Loss:    1.0398: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
      "Running Evaluation: 100%|██████████| 10/10 [00:01<00:00,  6.57it/s]\n",
      "d:\\Drive\\KULEEAH\\Semester 8\\TA Farihin\\env\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [00:28<00:00, 28.21s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6,\n",
       " {'global_step': [6],\n",
       "  'precision': [0.0],\n",
       "  'recall': [0.0],\n",
       "  'f1_score': [0.0],\n",
       "  'train_loss': [1.039774775505066],\n",
       "  'eval_loss': [0.7059326231479645]})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result, model_outputs, wrong_preds = model.eval_model(eval_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions, raw_outputs = model.predict(\r\n",
    "    # [\"@PolsekAlian PolriBerikan inovasi-inovasi Polri yang memudahkan masyarakat, kegiatan pemolisian masyarakat dan seluruh tindakan Kepolisian dalam menghadapi gangguan Kamtibmas, dan lain-lain - Jenderal Polisi Drs. Listyo Sigit Prabowo, https://t.co/f8GZ6VPqPD. - Kapolri #PolriTVRadioPresisi\"],\r\n",
    "    [\"Safari Ramadhan di Malang, Gubernur Khofifah ingatkan pentingnya Rembuk Nyekrup birokrat\"], \r\n",
    "    [\"@OPPOIndonesia ✍Butuh memori besar 128GB di OPPO A54 buat simpan file-file tugas kuliah dan testimoni online shop -ku ��Semoga oppo A54 bisa menunjang penghasilan keluargaku �� @OPPOIndonesia @nisa_rkt @Bebheey @kus_ica @Erna19_ @Saftriyuni #OPPOA54 https://t.co/bMYIiRb4QF �� 2.921\"]\r\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}