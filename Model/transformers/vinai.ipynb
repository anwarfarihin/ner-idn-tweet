{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from transformers import AutoModel, AutoTokenizer \r\n",
    "\r\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\r\n",
    "\r\n",
    "# For transformers v4.x+: \r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\r\n",
    "\r\n",
    "# For transformers v3.x: \r\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\r\n",
    "\r\n",
    "# INPUT TWEET IS ALREADY NORMALIZED!\r\n",
    "line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :crying_face:\"\r\n",
    "\r\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    features = bertweet(input_ids)  # Models outputs are now tuples\r\n",
    "    print(features)\r\n",
    "## With TensorFlow 2.0+:\r\n",
    "# from transformers import TFAutoModel\r\n",
    "# bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import pipeline\r\n",
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base\"\r\n",
    "sentiment_task = pipeline(\"ner\", model=model_path, tokenizer=model_path)\r\n",
    "sentiment_task(\"Huggingface es lo mejor! Awesome library 🤗😎\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 652/652 [00:00<00:00, 653kB/s]\n",
      "Downloading: 100%|██████████| 1.11G/1.11G [02:42<00:00, 6.85MB/s]\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 5.07M/5.07M [00:00<00:00, 6.72MB/s]\n",
      "Downloading: 100%|██████████| 9.10M/9.10M [00:04<00:00, 2.14MB/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.56214875,\n",
       "  'index': 1,\n",
       "  'word': '▁Hu',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5765965,\n",
       "  'index': 2,\n",
       "  'word': 'gging',\n",
       "  'start': 2,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5191802,\n",
       "  'index': 3,\n",
       "  'word': 'face',\n",
       "  'start': 7,\n",
       "  'end': 11},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.518767,\n",
       "  'index': 4,\n",
       "  'word': '▁es',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.57669675,\n",
       "  'index': 5,\n",
       "  'word': '▁lo',\n",
       "  'start': 15,\n",
       "  'end': 17},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.53062,\n",
       "  'index': 6,\n",
       "  'word': '▁mejor',\n",
       "  'start': 18,\n",
       "  'end': 23},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5114384,\n",
       "  'index': 7,\n",
       "  'word': '!',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.50668,\n",
       "  'index': 8,\n",
       "  'word': '▁Awesome',\n",
       "  'start': 25,\n",
       "  'end': 32},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5042591,\n",
       "  'index': 9,\n",
       "  'word': '▁libra',\n",
       "  'start': 33,\n",
       "  'end': 38},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5441581,\n",
       "  'index': 10,\n",
       "  'word': 'ry',\n",
       "  'start': 38,\n",
       "  'end': 40},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.50206345,\n",
       "  'index': 11,\n",
       "  'word': '▁',\n",
       "  'start': 41,\n",
       "  'end': 42},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.514811,\n",
       "  'index': 12,\n",
       "  'word': '🤗',\n",
       "  'start': 41,\n",
       "  'end': 42},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5115319,\n",
       "  'index': 13,\n",
       "  'word': '😎',\n",
       "  'start': 42,\n",
       "  'end': 43}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from transformers import pipeline\r\n",
    "model_path = \"vinai/bertweet-base\"\r\n",
    "sentiment_task = pipeline(\"ner\", model=model_path, tokenizer=model_path)\r\n",
    "sentiment_task(\"Huggingface es lo mejor! Awesome library 🤗😎\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.50157136,\n",
       "  'index': 1,\n",
       "  'word': 'Hu@@',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.5664712,\n",
       "  'index': 2,\n",
       "  'word': 'gging@@',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.6054892,\n",
       "  'index': 3,\n",
       "  'word': 'face',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5326702,\n",
       "  'index': 4,\n",
       "  'word': 'es',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5877536,\n",
       "  'index': 5,\n",
       "  'word': 'lo',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.6352468,\n",
       "  'index': 6,\n",
       "  'word': 'me@@',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.52928597,\n",
       "  'index': 7,\n",
       "  'word': 'jor@@',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5644406,\n",
       "  'index': 8,\n",
       "  'word': '!',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.51661885,\n",
       "  'index': 9,\n",
       "  'word': 'Awesome',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5510691,\n",
       "  'index': 10,\n",
       "  'word': 'library',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5734527,\n",
       "  'index': 11,\n",
       "  'word': '<unk>',\n",
       "  'start': None,\n",
       "  'end': None},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.5818505,\n",
       "  'index': 12,\n",
       "  'word': '<unk>',\n",
       "  'start': None,\n",
       "  'end': None}]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "95f69f4171822e9a0d4258a52ad5b962ce921882fa0fb93c07eb68a8992e9dab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}